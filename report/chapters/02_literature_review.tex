\chapter{Literature Review}
\section{Traffic forecasting foundations}
Traffic forecasting has evolved from classical time series methods to machine learning and deep learning approaches \cite{vlahogianni2014,ma2015}. Early work relied on autoregressive models and Kalman filtering, which assumed linear dynamics and required careful stationarity checks. With increasing data availability, statistical learning models such as random forests, gradient boosting, and support vector regression gained popularity due to their ability to capture non linear relationships. More recent studies employ recurrent neural networks and temporal convolutional networks to model complex temporal dependencies. In urban settings, the choice of model is shaped by data quality, sampling frequency, and the need for interpretable outputs. This project adopts tree based methods within a Spark ML pipeline because they are robust to heterogeneous features and provide stable performance without excessive tuning, which is suitable for an MSc project focused on end to end engineering.

The literature identifies a trade off between model complexity and operational feasibility. Deep learning models can outperform classical approaches when large sensor networks and high frequency data are available. However, they can be difficult to interpret and deploy in settings where data is sparse or irregular. Tree based methods have demonstrated strong performance in structured tabular contexts and require less feature normalization. This makes them attractive for open datasets that contain mixed data types and varying quality. The literature also emphasizes that model selection should reflect data characteristics rather than novelty. This review therefore prioritizes approaches that align with the available London datasets, while acknowledging advanced methods that could be explored in future work. The trade-off analysis is included to justify the design choices used in this dissertation.

A recurring theme is the importance of data preprocessing in forecasting accuracy. Studies highlight that normalization, consistent timestamp handling, and careful treatment of missing values can change model performance as much as algorithm choice. The practical implication is that forecasting success depends on the entire pipeline rather than on a single modeling step. This project reflects that view by focusing on pipeline reproducibility and data governance as key elements of the forecasting foundation.

\section{Spatiotemporal context in urban networks}
Urban traffic is inherently spatiotemporal because congestion propagates along connected road segments and varies across time of day, day of week, and seasonal patterns. The literature highlights the importance of spatial correlation, typically modeled through graph structures or adjacency matrices \cite{li2018}. Graph neural networks and spatial autoregressive models are common in academic research, but they often require dense sensor networks or advanced infrastructure. When sensor coverage is limited, spatial proxies such as node coordinates, road segment identifiers, and aggregated hotspot rates are used. This dissertation leverages a routing graph built from the road network and translates model outputs into spatial hotspot measures. This approach does not require complex neural architectures, but still captures spatial risk patterns through aggregation over edges and routing paths.

A key insight from the literature is the importance of spatial aggregation in the absence of full sensor coverage. Aggregating model outputs across edges or zones provides stable spatial indicators that can be visualized and interpreted. Studies also show that incorporating basic spatial information such as latitude and longitude improves predictive accuracy by allowing models to learn location specific effects. The present project adopts this principle and couples it with graph based routing to provide tangible spatial outputs. This combination offers a pragmatic balance between spatiotemporal modeling and operational feasibility.

The literature also notes that spatial context improves the interpretability of results. Planners and analysts often prefer outputs linked to recognizable corridors or junctions rather than abstract indices. By retaining spatial identifiers and converting outputs to geospatial layers, the project aligns with this requirement. This reinforces the idea that spatiotemporal modeling is not only a technical choice but also a communication strategy. This evidence supports prioritizing spatially interpretable outputs over opaque embeddings.

\section{Feature engineering for congestion prediction}
Feature engineering remains central to predictive accuracy in traffic modeling. The literature reports strong effects from temporal features (hour, day of week, month), calendar effects (holidays and school terms), and external factors (incidents and disruptions). Disruption data from transit agencies is widely used as a proxy for network stress. The project integrates Transport for London disruption data and maps it to hourly buckets. This creates a severity feature that augments traffic counts. Spatial features such as latitude and longitude of count points support geographic generalization and allow models to capture location specific demand. The decision to limit features to a small, explainable set reflects a balance between interpretability and performance.

Several studies highlight that feature selection must reflect both data availability and modeling goals. While richer features can improve accuracy, they can also introduce missingness and bias. For example, weather data can improve predictive performance but may require complex joins and quality checks. By focusing on a compact, stable feature set, the project ensures consistency across time and reduces the risk of model drift. The literature also notes that engineered features should align with the resolution of the prediction task. The hourly aggregation used here is consistent with the temporal granularity of both the traffic and disruption datasets, reducing misalignment between inputs and outputs.

The literature emphasizes that feature interpretability supports stakeholder trust. When features are easily explained, model outputs are more likely to be adopted in operational settings. This project therefore favors simple temporal and disruption features that align with stakeholder understanding of congestion drivers. This design choice supports both interpretability and model stability. The emphasis on interpretable features is used to reduce adoption risk in decision support contexts.

\section{Model evaluation and walk forward validation}
Evaluation design influences the validity of conclusions. Time series problems require temporal splits to avoid leakage. Walk forward validation, where training is performed on a sliding window and testing on future periods, is widely recommended. This dissertation implements walk forward evaluation to assess the ability of models to generalize over time. Metrics such as RMSE, MAE, and R2 are used for comparability with prior work. The literature indicates that improvements in RMSE can translate into meaningful operational benefits when applied to large traffic volumes, and this project uses these metrics to compare baseline and enhanced models.

Walk forward validation is particularly relevant for transport data because traffic patterns evolve with policy changes, infrastructure updates, and seasonal shifts. The literature emphasizes that random train test splits can overestimate performance by exposing the model to future information. The use of temporal splits is therefore essential for credible evaluation. This project follows these recommendations and records fold level metrics, which are then summarized for reporting. This approach aligns with best practice in applied forecasting studies and supports transparent interpretation of results.

The evaluation literature also highlights the importance of reporting multiple metrics. RMSE captures sensitivity to large errors, MAE reflects average absolute deviation, and R2 provides a variance explanation perspective. By reporting all three, the project aligns with established evaluation norms and facilitates comparison with prior studies. Multiple metrics are retained because each captures different error properties needed for operational use.

\section{Routing and hotspot analysis in decision support}
Routing analytics translates predictions into actionable insights. Studies on congestion aware routing compare shortest distance and travel time weighted paths, often incorporating penalties based on incident likelihood or predicted delay. Hotspot analysis typically uses spatial aggregation of high risk segments to identify priority areas. This dissertation builds a weighted routing graph using predicted hotspot rates, producing two route variants for comparison. The output is a set of route level metrics that quantify distance, congestion weighted cost, and hotspot exposure. These outputs align with decision support needs in urban planning and demonstrate the end value of predictive models beyond raw accuracy.

The literature suggests that hotspot identification can inform targeted interventions, such as signal timing changes or infrastructure upgrades. By computing hotspot rates at the edge level, the project generates interpretable indicators that can be compared across road segments. Routing comparisons further illustrate trade offs between distance and congestion exposure. This dual focus aligns with the literature on decision support, which emphasizes both descriptive and prescriptive analytics. The approach remains computationally tractable and can be extended with additional cost functions or constraints if required.

Decision support studies also emphasize transparency in route weighting. If congestion penalties are opaque, stakeholders may distrust route recommendations. By using explicit hotspot rates as weights, the project provides a clear rationale for route differences. This transparency aligns with best practices for applied analytics in public sector contexts. Explicit weighting is chosen to make route trade offs explainable to stakeholders.

\section{Visualization and geospatial dashboards}
Geospatial dashboards are crucial for communicating complex analytics to non technical stakeholders. Mapbox and similar platforms allow interactive visualization of routes, hotspots, and model outputs \cite{mapbox}. The literature emphasizes the importance of clear legends, filtering controls, and contextual metrics. This project implements a static Mapbox dashboard that loads GeoJSON layers and metrics tables generated by the pipeline. The design focuses on interpretability, with toggles for hotspots and route selection. By separating analytics from visualization, the system supports reproducibility and reduces coupling between model changes and front end development.

Prior work highlights that dashboards should present the minimal set of controls necessary for meaningful exploration. Excessive complexity can reduce usability and obscure insights. The present dashboard therefore prioritizes a small set of filters and metrics that directly relate to the research questions. The literature also points to the importance of spatial context, including base maps and color scales that align with the meaning of the data. The design choices in the dashboard align with these principles, supporting clear communication of congestion risk and routing alternatives.

The literature on visualization also notes that static dashboards can be sufficient when data refresh cycles are periodic rather than real time. This project adopts a static approach to reduce operational overhead while still providing interactive exploration. This aligns with the goal of producing a deployable and maintainable output for an MSc project. Static deployment is selected because it meets the communication needs without adding server maintenance.

The reviewed literature collectively emphasizes that successful traffic analytics depends on three aligned layers: data readiness, model suitability, and communication of outputs. Data readiness involves consistent preprocessing, temporal alignment, and clear handling of missingness. Model suitability requires approaches that match the scale and structure of available datasets, with practical interpretability for operational use. Communication of outputs is achieved through spatial aggregation and visual dashboards that contextualize model results within the road network. These themes reinforce the selection of a pipeline based on structured features and graph based post processing. They also highlight why performance metrics alone are insufficient to evaluate applied traffic analytics. A model can be accurate yet unusable if outputs cannot be translated into decisions. Conversely, a modestly accurate model may still provide valuable insights if it reveals persistent congestion patterns and supports route comparisons. The literature therefore justifies the end to end framing of this dissertation and motivates the inclusion of spatial outputs and dashboard deployment as integral parts of the research contribution.

Another insight from the literature is the importance of methodological transparency for public sector analytics. Studies in urban computing emphasize that stakeholders are more likely to adopt analytics when assumptions and data sources are explicit. This is especially relevant for congestion management, where interventions can affect multiple communities and attract public scrutiny. Transparent pipelines, clear documentation, and reproducible outputs therefore become part of the analytical contribution. The reviewed work also suggests that open data analytics should prioritize robustness and repeatability over marginal accuracy gains, because data updates and policy changes require frequent re execution. These insights reinforce the decision to build a modular pipeline with repeatable processing stages rather than focusing solely on model experimentation. They also strengthen the argument that evaluation should include both predictive performance and the utility of spatial outputs in real decision contexts.

Finally, the literature indicates that applied transport analytics benefits from alignment with stakeholder workflows. Decision makers often work with static reports or periodic dashboards rather than continuous streams. This means that batch pipelines and scheduled updates can be more appropriate than real time systems, provided they deliver clear and timely insights. The reviewed studies show that when dashboards include simple controls and summary metrics, they are more likely to be adopted. This supports the design of a static Mapbox dashboard and a pipeline that produces stable exports. The literature therefore provides not only methodological guidance but also practical design cues that shape how results should be delivered.

The synthesis across studies suggests that scalable analytics should favor pipelines that are easy to rerun and audit. In transport applications, data updates are frequent and policy contexts evolve, so reproducibility is a prerequisite for credibility. This reinforces the emphasis on deterministic preprocessing and versioned outputs in the project design. It also justifies the focus on interpretable models, where assumptions can be communicated clearly to stakeholders without specialized machine learning expertise.

The literature also points to the value of incremental improvement over single large model changes. Many transport studies report gains from small refinements in feature engineering or evaluation design. This aligns with the iterative development approach used in the project, where improvements are validated through controlled experiments and retained only if they improve both accuracy and interpretability.

Across the reviewed work, there is a consistent call for evaluation frameworks that reflect deployment conditions. This reinforces the use of temporal splits and walk forward testing in the dissertation. It also supports the decision to report both error metrics and spatial outputs, which together provide a richer basis for applied interpretation.
Figure~\ref{fig:lit-themes} summarizes the core themes that shape the methodological choices in this dissertation.

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=1.0cm and 0.9cm]
  \node[block] (forecast) {Forecasting\\models};
  \node[block, right=of forecast] (spatial) {Spatial\\context};
  \node[block, right=of spatial] (decision) {Decision\\support};
  \node[block, below=of spatial] (synthesis) {Pipeline design\\for this study};
  \draw[line] (forecast) -- (synthesis);
  \draw[line] (spatial) -- (synthesis);
  \draw[line] (decision) -- (synthesis);
\end{tikzpicture}
\caption{Summary of literature themes: forecasting models, spatial context, and decision support.}
\label{fig:lit-themes}
\end{figure}
